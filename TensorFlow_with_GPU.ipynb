{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fhariyaaaaa/Analysis-and-prediction-of-homevalues/blob/main/TensorFlow_with_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMce8muBqXQP"
      },
      "source": [
        "# Tensorflow with GPU\n",
        "\n",
        "This notebook provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. In this notebook you will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU, observing the speedup provided by using the GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM_8ELnJq_wd"
      },
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "sXnDmXR7RDr2",
        "outputId": "230c8c7a-13fd-46b3-c5b1-9c6330cfd846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3fE7KmKRDsH"
      },
      "source": [
        "## Observe TensorFlow speedup on GPU relative to CPU\n",
        "\n",
        "This example constructs a typical convolutional neural network layer over a\n",
        "random image and manually places the resulting ops on either the CPU or the GPU\n",
        "to compare execution speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Y04m-jvKRDsJ",
        "outputId": "bb32a619-b9f8-4866-fe6a-40d8c5341e7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "3.862475891000031\n",
            "GPU (s):\n",
            "0.10837535100017703\n",
            "GPU speedup over CPU: 35x\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "\n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKCOygkP_Sko",
        "outputId": "0f030ce6-a7ff-47f4-b0d5-6ff9e8774c96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "id": "nfwmPhyC_iPC",
        "outputId": "44bbba7e-cb61-458f-eafd-e83796b61074",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (10.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdfplumber\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# Step 1: Text Extraction from PDFs\n",
        "\n",
        "def extract_text_from_pdfs(pdf_folder: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Extract text from all PDF files in the given folder.\n",
        "\n",
        "    Args:\n",
        "        pdf_folder (str): Path to the folder containing PDF files.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: A list of dictionaries, each containing the file name, page number, and extracted text.\n",
        "    \"\"\"\n",
        "    extracted_text = []\n",
        "    for filename in os.listdir(pdf_folder):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(pdf_folder, filename)\n",
        "            try:\n",
        "                with pdfplumber.open(file_path) as pdf:\n",
        "                    for page_num, page in enumerate(pdf.pages):\n",
        "                        text = page.extract_text()\n",
        "                        if text:\n",
        "                            extracted_text.append({\n",
        "                                \"file_name\": filename,\n",
        "                                \"page_number\": page_num + 1,\n",
        "                                \"text\": text\n",
        "                            })\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {filename}: {e}\")\n",
        "    return extracted_text"
      ],
      "metadata": {
        "id": "Ozt0JI5JJjS8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Chunking Text with Metadata\n",
        "import re\n",
        "\n",
        "def chunk_text_with_metadata(extracted_text: List[Dict], chunk_size: int = 2000) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Chunk the extracted text into larger, meaningful sections with metadata.\n",
        "\n",
        "    Args:\n",
        "        extracted_text (List[Dict]): A list of dictionaries containing extracted text with metadata.\n",
        "        chunk_size (int): The maximum character length for each chunk.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: A list of dictionaries, each containing the file name, page number, and chunked text.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    for entry in extracted_text:\n",
        "        text = entry[\"text\"]\n",
        "        current_chunk = \"\"\n",
        "        for line in text.split(\"\\n\"):\n",
        "            # Skip lines that are boilerplate or procedural\n",
        "            if re.search(r'\\b(APPEARANCES|COUNSEL PRESENT|INDEX|EXHIBIT|COURT|TRANSCRIPT)\\b', line, re.IGNORECASE):\n",
        "                continue\n",
        "\n",
        "            if len(current_chunk) + len(line) <= chunk_size:\n",
        "                current_chunk += line + \" \"\n",
        "            else:\n",
        "                chunks.append({\n",
        "                    \"file_name\": entry[\"file_name\"],\n",
        "                    \"page_number\": entry[\"page_number\"],\n",
        "                    \"chunk_text\": current_chunk.strip()\n",
        "                })\n",
        "                current_chunk = line + \" \"\n",
        "\n",
        "        # Add the last chunk if it's not empty\n",
        "        if current_chunk:\n",
        "            chunks.append({\n",
        "                \"file_name\": entry[\"file_name\"],\n",
        "                \"page_number\": entry[\"page_number\"],\n",
        "                \"chunk_text\": current_chunk.strip()\n",
        "            })\n",
        "\n",
        "    return chunks\n",
        "\n"
      ],
      "metadata": {
        "id": "GJ--VFxiSsxT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3 : Cleaning\n",
        "def clean_text(chunks: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Clean the text by removing non-ASCII characters, extra whitespace, and short chunks.\n",
        "\n",
        "    Args:\n",
        "        chunks (List[Dict]): A list of dictionaries containing chunked text with metadata.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: A list of dictionaries with cleaned text.\n",
        "    \"\"\"\n",
        "    cleaned_chunks = []\n",
        "    for chunk in chunks:\n",
        "        cleaned_text = re.sub(r'[^\\x00-\\x7F]+', ' ', chunk[\"chunk_text\"])  # Remove non-ASCII characters\n",
        "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()  #remove extra spaces\n",
        "        if len(cleaned_text) > 50:\n",
        "            chunk[\"chunk_text\"] = cleaned_text\n",
        "            cleaned_chunks.append(chunk)\n",
        "    return cleaned_chunks\n"
      ],
      "metadata": {
        "id": "Yyu5QjCUSsii"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Function to Execute All Steps\n",
        "def preprocess_pdfs(pdf_folder: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Preprocess PDF files by extracting, chunking, and cleaning text.\n",
        "\n",
        "    Args:\n",
        "        pdf_folder (str): Path to the folder containing PDF files.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: A list of dictionaries containing cleaned, chunked text with metadata.\n",
        "    \"\"\"\n",
        "    extracted_text = extract_text_from_pdfs(pdf_folder)\n",
        "    chunks_with_metadata = chunk_text_with_metadata(extracted_text)\n",
        "    cleaned_chunks = clean_text(chunks_with_metadata)\n",
        "    return cleaned_chunks\n",
        "\n",
        "# Example Usage - text chunking\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_folder = \"/content\"\n",
        "    processed_chunks = preprocess_pdfs(pdf_folder)\n",
        "    for chunk in processed_chunks[:5]:\n",
        "        print(chunk)\n",
        "        print(\"\\n---\\n\")"
      ],
      "metadata": {
        "id": "Y3SEICsgQ_bb",
        "outputId": "8abf1d79-d350-4c65-eb74-a32a39403855",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'file_name': '2006-08-29 Basile v. Honda_Depo Transcript of Paul LeCour.pdf', 'page_number': 1, 'chunk_text': '0001 PENNSYLVANIA 2 - - - 3 KATHERINE M. BASILE, the Executrix)CIVIL DIVISION of the Estate of FRED DALBO, SR., )ASBESTOS 4 Deceased, and VIOLA IMOGENE COEN ) DALBO, his wife, in her own right,)NO. 5 )11484 CD 2005 Plaintiffs, ) 6 ) vs. ) 7 ) AMERICAN HONDA MOTOR COMPANY, ) 8 INC., et al., ) ) 9 Defendants. ) 10 - - - 11 Video Deposition of PAUL LeCOUR 12 Tuesday, August 29, 2006 13 - - - 14 The video deposition of PAUL LeCOUR, called as a witness on behalf of the Plaintiff, pursuant 15 to notice and the Pennsylvania Rules of Civil Procedure pertaining to the taking of 16 depositions, taken before me, the undersigned, Terri J. Urbash, a Notary Public in and for the 17 Commonwealth of Pennsylvania, held at RoData, 1207 Muriel Street, Pittsburgh, Pennsylvania 18 15203, commencing at 9:32 a.m., the day and date above set forth. 19 - - - 20 21 - - - 22 NETWORK DEPOSITION SERVICES 247 FORT PITT BOULEVARD, SUITE 300 23 PITTSBURGH, PENNSYLVANIA 15222 412-281-7908 24 - - - 25'}\n",
            "\n",
            "---\n",
            "\n",
            "{'file_name': '2006-08-29 Basile v. Honda_Depo Transcript of Paul LeCour.pdf', 'page_number': 2, 'chunk_text': \"002 2 On behalf of the Plaintiffs: 3 Savinis, D'Amico & Kane, LLP: Michael Gallucci, Esquire 4 Suite 3626, Gulf Tower Pittsburgh, Pennsylvania 15219 5 On behalf of the Defendant Bridgestone 6 Firestone: (via telephone) 7 Edward Paul, Esquire 320 West Front Street 8 Media, Pennsylvania 19063 9 On behalf of the Defendant Borg-Warner Corporation: 10 Barnard, Mezzanotte and Pinnie: 11 Timothy B. Barnard, Esquire The Williamson House 12 218 West Front Street P.O. Box 289 13 Media, Pennsylvania 19063 14 On behalf of the Defendant Gould Pumps, Inc.: 15 Grogan Graffam, P.C.: 16 Edward J. Chiodo, Esquire Four Gateway Center, 12th Floor 17 Pittsburgh, Pennsylvania 15222 18 On behalf of the Defendant Kelsey-Hayes: 19 Wilbraham, Lawler & Buba: Jennifer E. Watson, Esquire 20 Two Gateway Center, 17 North Pittsburgh, Pennsylvania 15222 21 On behalf of the Defendant Pneumo-Abex: 22 Swartz Campbell, LLC: 23 Anne L. Wilcox, Esquire 4750 U.S. Steel Tower 24 Pittsburgh, Pennsylvania 15219 25\"}\n",
            "\n",
            "---\n",
            "\n",
            "{'file_name': '2006-08-29 Basile v. Honda_Depo Transcript of Paul LeCour.pdf', 'page_number': 3, 'chunk_text': '003 2 On behalf of the Defendant Garlock Sealing Technologies, LLC: 3 Margolis Edelstein: 4 Gregory L. Fitzpatrick, Esquire The Grant Building, Suite 1500 5 Pittsburgh, Pennsylvania 15219 6 On behalf of the Defendant Genuine Parts Company: 7 Riley, Hewitt, Witte & Romano, P.C.: 8 Patrick Riley, Esquire 650 Washington Road, Suite 300 9 Pittsburgh, Pennsylvania 15228 10 On behalf of the Defendant Parker-Hannifan Corporation: 11 Cohen & Grigsby, P.C.: 12 Patrick G. Barry, Esquire 11 Stanwix Street, 15th Floor 13 Pittsburgh, Pennsylvania 15222 14 On behalf of the Defendant Sears Roebuck Company: 15 Hawkins & Parnell: 16 Elaine Shofner, Esquire 400 SunTrust Plaza 17 303 Peachtree Street, N.E. Atlanta, Georgia 30308 18 On behalf of the Defendant Honeywell, Inc.: 19 King & Spalding: 20 Rahmah A. Abdulaleem, Esquire 1180 Peachtree Street, N.E. 21 Atlanta, Georgia 30309 22 23 24 25'}\n",
            "\n",
            "---\n",
            "\n",
            "{'file_name': '2006-08-29 Basile v. Honda_Depo Transcript of Paul LeCour.pdf', 'page_number': 4, 'chunk_text': \"004 2 On behalf of the Defendant Pneumo Abex: 3 DeHay & Elliston, LLP: Thomas Burch, Esquire 4 3700 Buffalo Speedway Suite 1000, Tenth Floor 5 Houston, Texas 77098 6 On behalf of the Defendant Genuine Parts Company: 7 Alston & Bird, LLP: 8 J. Kennard Neal, Esquire One Atlantic Center 9 1201 West Peachtree Street Atlanta, Georgia 30309 10 On behalf of the Defendant American Honda 11 Motor Company: 12 Lavin, O'Neil, Ricci, Cedrone & Disipio: Edward Finch, Esquire 13 190 North Independence Mall West Suite 500 14 Philadelphia, Pennsylvania 19106 15 On behalf of the Defendant Caterpillar, Inc.: 16 Burns, White & Hickton, LLC: 17 Jeff Roberts, Esquire Four Northshore Center 18 106 Isabella Street Pittsburgh, Pennsylvania 15212 19 On behalf of the Defendant Ford Motor 20 Company General Motors Corporation: 21 Eckert Seamans Cherin & Mellott, LLC: Eric Horne, Esquire 22 44th Floor, U.S. Steel Tower Pittsburgh, Pennsylvania 15219 23 - - - 24 25\"}\n",
            "\n",
            "---\n",
            "\n",
            "{'file_name': '2006-08-29 Basile v. Honda_Depo Transcript of Paul LeCour.pdf', 'page_number': 5, 'chunk_text': \"005 1 I-N-D-E-X P-A-G-E 1 - 7/26/06 letter to Mr. Martucci 12 3 2 - GPC's responses to interrogatories 17 3 - GPC's responses to second set of 4 interrogatories 17 4 - copy of document 65 5 5 - copy of color picture 75 6 - American Brakeblok specifications 79 6 7 - 11/16/90 memo to Charles Barker et al. from Paul LeCour 86 7 8 - copy of article 117 9 - copy of handwritten document 165 8 10 - 12/7/87 letter to Mr. White from Mr. Shepard 165 9 11 - 10/26/71 letter to Mr. Childress from Mr. Kennebeck 177 10 12 - copy of U.S. Department of Labor violation citation 187 11 - - - EXAMINATION BY: PAGE 12 Mr. Gallucci - 5/197 Ms. Abdulaleen - 190 13 Mr. Riley - 193 - - - 14 SIGNATURE LETTER - 203 15 - - - 16 17 18 19 20 21 22 23 24 25\"}\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9XlbHKyhpckk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "TensorFlow with GPU",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}